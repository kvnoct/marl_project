{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from model import Policy\n",
    "from storage import RolloutStorage\n",
    "from wrappers import  TimeLimit\n",
    "from a2c import A2C\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env, time_limit = 5):\n",
    "    env = TimeLimit(env, time_limit)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"rware:rware-tiny-2ag-v1\")\n",
    "writer = SummaryWriter(\"test\")\n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "agents = [A2C(i, obs_space, action_space) for i in range(env.n_agents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlab\\AppData\\Local\\Temp\\ipykernel_17696\\1461834541.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(n_log_probs[i]),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n",
      "rollout finished\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Get the initial obs\n",
    "for i in range(len(obs)):\n",
    "    agents[i].storage.obs[0].copy_(torch.tensor(obs[i]))\n",
    "    agents[i].storage.to('cpu')\n",
    "\n",
    "\n",
    "for j in range(100):\n",
    "    #N-steps rollout\n",
    "    for _ in range(5):\n",
    "        #Get the action\n",
    "        with torch.no_grad():\n",
    "            n_value, n_action, n_log_probs = zip( *[agent.model.act(agent.storage.obs[0]) for agent in agents] )\n",
    "            n_action = [action.item() for action in n_action]\n",
    "\n",
    "        #Step on env\n",
    "        obs, reward, done, infos = env.step(n_action)\n",
    "        \n",
    "        #Get the mask\n",
    "        masks = torch.tensor([[0.0] if done_ else [1.0] for done_ in done])\n",
    "\n",
    "        #Copy the state transition to agent's on-policy storage\n",
    "        for i in range(len(agents)):\n",
    "            agents[i].storage.insert(torch.tensor(obs[i]),\n",
    "                                    torch.tensor(n_action[i]),\n",
    "                                    torch.tensor(n_log_probs[i]), \n",
    "                                    n_value[i],\n",
    "                                    torch.tensor(reward[i]),\n",
    "                                    masks[i])\n",
    "\n",
    "    print('rollout finished')\n",
    "    for agent in agents:\n",
    "        agent.compute_returns()\n",
    "\n",
    "    for agent in agents:\n",
    "        loss = agent.update([a.storage for a in agents])\n",
    "        for k, v in loss.items():\n",
    "            writer.add_scalar(f\"agent{agent.agent_id}/{k}\", v, j)\n",
    "\n",
    "    for agent in agents:\n",
    "        agent.storage.after_update()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'policy_loss': -1.442805528640747,\n",
       " 'value_loss': 0.3882187306880951,\n",
       " 'dist_entropy': 0.016044992208480834,\n",
       " 'importance_sampling': 1.001254916191101,\n",
       " 'seac_policy_loss': -1.5284276008605957,\n",
       " 'seac_value_loss': 0.4578359127044678}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[6., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[6., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[6., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[6., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[6., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[6., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents[0].storage.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents[0].storage.obs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents[0].storage.obs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
